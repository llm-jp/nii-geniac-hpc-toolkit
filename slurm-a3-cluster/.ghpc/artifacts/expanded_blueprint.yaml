# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

blueprint_name: slurm-a3-cluster
ghpc_version: v1.31.1-0-gb830db48
validation_level: 1
vars:
  a3_maintenance_interval: ""
  deployment_name: slurm-a3-cluster
  disk_size_gb: 200
  enable_cleanup_compute: true
  enable_cleanup_subscriptions: true
  enable_reconfigure: true
  final_image_family: slurm-dlvm
  labels:
    ghpc_blueprint: slurm-a3-cluster
    ghpc_deployment: ((var.deployment_name))
  local_mount_homefs: /home
  network_name_system: slurm-sys-net
  project_id: nii-geniac
  region: asia-southeast1
  remote_mount_homefs: /nfsshare
  server_ip_homefs: 10.167.145.130
  slurm_cluster_name: slurm0
  subnetwork_name_system: slurm-sys-subnet
  subnetwork_self_link_gpunet0: (("https://www.googleapis.com/compute/v1/projects/${var.project_id}/regions/${var.region}/subnetworks/slurm-gpu-subnet0"))
  subnetwork_self_link_gpunet1: (("https://www.googleapis.com/compute/v1/projects/${var.project_id}/regions/${var.region}/subnetworks/slurm-gpu-subnet1"))
  subnetwork_self_link_gpunet2: (("https://www.googleapis.com/compute/v1/projects/${var.project_id}/regions/${var.region}/subnetworks/slurm-gpu-subnet2"))
  subnetwork_self_link_gpunet3: (("https://www.googleapis.com/compute/v1/projects/${var.project_id}/regions/${var.region}/subnetworks/slurm-gpu-subnet3"))
  zone: asia-southeast1-c
  zones:
    - ((var.zone))
deployment_groups:
  - group: build_script
    terraform_backend:
      type: gcs
      configuration:
        bucket: geniac-tf-state-bucket
        prefix: (("slurm-a3-cluster/${var.deployment_name}/build_script"))
    modules:
      - source: modules/network/pre-existing-vpc
        kind: terraform
        id: sysnet
        outputs:
          - name: subnetwork_self_link
            description: Automatically-generated output exported for use by later deployment groups
            sensitive: true
          - name: network_self_link
            description: Automatically-generated output exported for use by later deployment groups
            sensitive: true
          - name: subnetwork_name
            description: Automatically-generated output exported for use by later deployment groups
            sensitive: true
        settings:
          network_name: ((var.network_name_system))
          project_id: ((var.project_id))
          region: ((var.region))
          subnetwork_name: ((var.subnetwork_name_system))
      - source: modules/scripts/startup-script
        kind: terraform
        id: image_build_script
        outputs:
          - name: startup_script
            description: Automatically-generated output exported for use by later deployment groups
            sensitive: true
        settings:
          configure_ssh_host_patterns:
            - 10.0.0.*
            - 10.1.0.*
            - 10.2.0.*
            - 10.3.0.*
            - (("${var.slurm_cluster_name}*"))
          deployment_name: ((var.deployment_name))
          install_ansible: true
          labels: ((var.labels))
          project_id: ((var.project_id))
          region: ((var.region))
          runners:
            - content: |
                #!/bin/bash
                set -e -o pipefail
                rm -f /etc/apt/sources.list.d/kubernetes.list
                apt-get update --allow-releaseinfo-change
              destination: workaround_apt_change.sh
              type: shell
            - content: |
                #!/bin/bash
                # many extra services are being started via /etc/rc.local; disable
                # them on future boots of image
                echo -e '#!/bin/bash\n/usr/bin/nvidia-persistenced --user root\nexit 0' > /etc/rc.local
                # disable jupyter and notebooks-collection-agent services
                systemctl stop jupyter.service notebooks-collection-agent.service
                systemctl disable jupyter.service notebooks-collection-agent.service
              destination: disable_dlvm_builtin_services.sh
              type: shell
            - content: |
                #!/bin/bash
                # apt-get update is run above; add here if needed
                apt-get install --assume-yes libpmix-dev
              destination: install_pmix.sh
              type: shell
            - content: |
                {
                  "reboot": false,
                  "slurm_version": "23.02.7",
                  "install_cuda": false,
                  "nvidia_version": "latest",
                  "install_ompi": true,
                  "install_lustre": true,
                  "install_gcsfuse": true,
                  "monitoring_agent": "cloud-ops"
                }
              destination: /var/tmp/slurm_vars.json
              type: data
            - content: |
                #!/bin/bash
                set -e -o pipefail
                ansible-galaxy role install googlecloudplatform.google_cloud_ops_agents
                ansible-pull \
                    -U https://github.com/GoogleCloudPlatform/slurm-gcp -C 5.10.4 \
                    -i localhost, --limit localhost --connection=local \
                    -e @/var/tmp/slurm_vars.json \
                    ansible/playbook.yml
              destination: install_slurm.sh
              type: shell
            - content: |
                * - memlock unlimited
                * - nproc unlimited
                * - stack unlimited
                * - nofile 1048576
                * - cpu unlimited
                * - rtprio unlimited
              destination: /etc/security/limits.d/99-unlimited.conf
              type: data
            - content: |
                [Service]
                LimitNOFILE=infinity
              destination: /etc/systemd/system/slurmd.service.d/file_ulimit.conf
              type: data
            - content: |
                [Unit]
                Description=Delay A3 boot until all network interfaces are routable
                After=network-online.target
                Wants=network-online.target
                Before=google-startup-scripts.service

                [Service]
                ExecCondition=/bin/bash -c '/usr/bin/curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/machine-type | grep -q "/a3-highgpu-8g$"'
                ExecStart=/usr/lib/systemd/systemd-networkd-wait-online -i enp6s0 -i enp12s0 -i enp134s0 -i enp140s0 -o routable --timeout=120
                ExecStartPost=/bin/sleep 60

                [Install]
                WantedBy=multi-user.target
              destination: /etc/systemd/system/delay-a3.service
              type: data
            - content: |
                #!/bin/bash
                set -e -o pipefail
                # allow any user to use Docker
                DOCKER_SOCKET_DROPIN_DIR="/etc/systemd/system/docker.socket.d"
                if [ ! -d "${DOCKER_SOCKET_DROPIN_DIR}" ]; then
                    mkdir "${DOCKER_SOCKET_DROPIN_DIR}"
                fi
                DOCKER_SOCKET_DROPIN_FILE="group-override.conf"
                if [ ! -f "${DOCKER_SOCKET_DROPIN_DIR}/${DOCKER_SOCKET_DROPIN_FILE}" ]; then
                    # indentation matters in EOT below; do not blindly edit!
                    cat <<EOT > "${DOCKER_SOCKET_DROPIN_DIR}/${DOCKER_SOCKET_DROPIN_FILE}"
                [Socket]
                SocketMode=0666
                EOT
                fi
              destination: configure_docker.sh
              type: shell
            - content: |
                #!/bin/bash
                set -e -o pipefail
                ### Setting up Enroot
                if ! dpkg -l enroot &>/dev/null; then
                    arch=\$(dpkg --print-architecture)
                    curl -fSsL -O https://github.com/NVIDIA/enroot/releases/download/v3.4.1/enroot_3.4.1-1_${arch}.deb
                    curl -fSsL -O https://github.com/NVIDIA/enroot/releases/download/v3.4.1/enroot+caps_3.4.1-1_${arch}.deb # optional
                    apt-get update
                    apt-get install --assume-yes ./*.deb
                    rm enroot*.deb
                fi
                # configure enroot
                # use single quotes around EOT to avoid shell interpolation
                cat <<'EOT' > /etc/enroot/enroot.conf
                ENROOT_RUNTIME_PATH    /mnt/localssd/${UID}/enroot/runtime
                ENROOT_CACHE_PATH      /mnt/localssd/${UID}/enroot/cache
                ENROOT_DATA_PATH       /mnt/localssd/${UID}/enroot/data
                EOT
                ### Install Pyxis
                if [ ! -f "/usr/local/lib/slurm/spank_pyxis.so" ]; then
                    git clone --depth 1 https://github.com/NVIDIA/pyxis.git
                    cd pyxis && make install && cd -
                    rm -rf pyxis
                    echo "required /usr/local/lib/slurm/spank_pyxis.so" > /etc/slurm/plugstack.conf
                fi
              destination: install_enroot_pyxis.sh
              type: shell
            - content: |
                #!/bin/bash
                apt-get update
                apt-get install mdadm --no-install-recommends --assume-yes
              destination: install_mdadm.sh
              type: shell
            - content: |
                #!/bin/bash
                set -e -o pipefail

                RAID_DEVICE=/dev/md0
                DST_MNT=/mnt/localssd
                DISK_LABEL=LOCALSSD
                OPTIONS=discard,defaults

                # if mount is successful, do nothing
                if mount --source LABEL="$DISK_LABEL" --target="$DST_MNT" -o "$OPTIONS"; then
                        exit 0
                fi

                # Create new RAID, format ext4 and mount
                # TODO: handle case of zero or 1 local SSD disk
                # TODO: handle case when /dev/md0 exists but was not mountable for
                # some reason
                DEVICES=`nvme list | grep nvme_ | grep -v nvme_card-pd | awk '{print $1}' | paste -sd ' '`
                NB_DEVICES=`nvme list | grep nvme_ | grep -v nvme_card-pd | awk '{print $1}' | wc -l`
                mdadm --create "$RAID_DEVICE" --level=0 --raid-devices=$NB_DEVICES $DEVICES
                mkfs.ext4 -F "$RAID_DEVICE"
                tune2fs "$RAID_DEVICE" -r 131072
                e2label "$RAID_DEVICE" "$DISK_LABEL"
                mkdir -p "$DST_MNT"
                mount --source LABEL="$DISK_LABEL" --target="$DST_MNT" -o "$OPTIONS"
                chmod 1777 "$DST_MNT"
              destination: /usr/local/ghpc/mount_localssd.sh
              type: data
            - content: |
                [Unit]
                Description=Assemble local SSDs as software RAID; then format and mount

                [Service]
                ExecCondition=bash -c '/usr/bin/curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/machine-type | grep -q "/a3-highgpu-8g$"'
                ExecStart=/bin/bash /usr/local/ghpc/mount_localssd.sh

                [Install]
                WantedBy=local-fs.target
              destination: /etc/systemd/system/mount-local-ssd.service
              type: data
            - content: |
                #!/bin/bash
                set -e -o pipefail
                # workaround b/309016676 (systemd-resolved restarts 4 times causing DNS
                # resolution failures during google-startup-scripts.service)
                systemctl daemon-reload
                systemctl enable delay-a3.service
                systemctl enable mount-local-ssd.service
              destination: systemctl_services.sh
              type: shell
            - content: |
                #!/bin/bash
                # THIS RUNNER MUST BE THE LAST RUNNER BECAUSE IT WILL BREAK GSUTIL IN
                # PARENT SCRIPT OF STARTUP-SCRIPT MODULE
                set -e -o pipefail
                # Remove original DLVM gcloud, lxds install due to conflict with snapd and NFS
                snap remove google-cloud-cli lxd
                # Install key and google-cloud-cli from apt repo
                GCLOUD_APT_SOURCE="/etc/apt/sources.list.d/google-cloud-sdk.list"
                if [ ! -f "${GCLOUD_APT_SOURCE}" ]; then
                    # indentation matters in EOT below; do not blindly edit!
                    cat <<EOT > "${GCLOUD_APT_SOURCE}"
                deb [signed-by=/usr/share/keyrings/cloud.google.asc] https://packages.cloud.google.com/apt cloud-sdk main
                EOT
                fi
                curl -o /usr/share/keyrings/cloud.google.asc https://packages.cloud.google.com/apt/doc/apt-key.gpg
                apt-get update
                apt-get install --assume-yes google-cloud-cli
                # Clean up the bash exceutable hash for subsequent steps using gsutil
                hash -r
              destination: remove_snap_gcloud.sh
              type: shell
  - group: slurm-build
    terraform_backend:
      type: gcs
      configuration:
        bucket: geniac-tf-state-bucket
        prefix: (("slurm-a3-cluster/${var.deployment_name}/slurm-build"))
    modules:
      - source: modules/packer/custom-image
        kind: packer
        id: slurm-image
        use:
          - image_build_script
          - sysnet
        settings:
          deployment_name: ((var.deployment_name))
          disk_size: ((var.disk_size_gb))
          image_family: ((var.final_image_family))
          labels: ((var.labels))
          machine_type: c2d-standard-32
          project_id: ((var.project_id))
          source_image: dlvm-tcpd-cu120-20231203-1800-rc0-ubuntu-2004-py310
          source_image_project_id:
            - deeplearning-platform
          startup_script: ((module.image_build_script.startup_script))
          subnetwork_name: ((module.sysnet.subnetwork_name))
          zone: ((var.zone))
  - group: cluster
    terraform_backend:
      type: gcs
      configuration:
        bucket: geniac-tf-state-bucket
        prefix: (("slurm-a3-cluster/${var.deployment_name}/cluster"))
    modules:
      - source: modules/file-system/pre-existing-network-storage
        kind: terraform
        id: homefs
        settings:
          local_mount: ((var.local_mount_homefs))
          remote_mount: ((var.remote_mount_homefs))
          server_ip: ((var.server_ip_homefs))
      - source: community/modules/compute/schedmd-slurm-gcp-v5-node-group
        kind: terraform
        id: debug_node_group
        settings:
          disk_size_gb: ((var.disk_size_gb))
          instance_image:
            family: ((var.final_image_family))
            project: ((var.project_id))
          instance_image_custom: true
          labels: ((var.labels))
          machine_type: n2-standard-8
          node_count_dynamic_max: 4
          node_count_static: 0
          project_id: ((var.project_id))
      - source: community/modules/compute/schedmd-slurm-gcp-v5-partition
        kind: terraform
        id: debug_partition
        use:
          - debug_node_group
          - sysnet
          - homefs
        settings:
          deployment_name: ((var.deployment_name))
          enable_placement: false
          enable_reconfigure: ((var.enable_reconfigure))
          exclusive: false
          network_storage: ((flatten([module.homefs.network_storage])))
          node_groups: ((flatten([module.debug_node_group.node_groups])))
          partition_name: debug
          project_id: ((var.project_id))
          region: ((var.region))
          slurm_cluster_name: ((var.slurm_cluster_name))
          subnetwork_self_link: ((module.sysnet.subnetwork_self_link))
          zone: ((var.zone))
          zones: ((var.zones))
      - source: community/modules/compute/schedmd-slurm-gcp-v5-node-group
        kind: terraform
        id: a3_node_group
        settings:
          additional_disks:
            - auto_delete: true
              boot: false
              device_name: local-ssd-0
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-1
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-2
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-3
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-4
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-5
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-6
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-7
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-8
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-9
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-10
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-11
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-12
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-13
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-14
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
            - auto_delete: true
              boot: false
              device_name: local-ssd-15
              disk_labels: {}
              disk_name: null
              disk_size_gb: 375
              disk_type: local-ssd
          additional_networks:
            - access_config: []
              alias_ip_range: []
              ipv6_access_config: []
              network: null
              network_ip: ""
              nic_type: GVNIC
              queue_count: null
              stack_type: IPV4_ONLY
              subnetwork: ((var.subnetwork_self_link_gpunet0))
              subnetwork_project: ((var.project_id))
            - access_config: []
              alias_ip_range: []
              ipv6_access_config: []
              network: null
              network_ip: ""
              nic_type: GVNIC
              queue_count: null
              stack_type: IPV4_ONLY
              subnetwork: ((var.subnetwork_self_link_gpunet1))
              subnetwork_project: ((var.project_id))
            - access_config: []
              alias_ip_range: []
              ipv6_access_config: []
              network: null
              network_ip: ""
              nic_type: GVNIC
              queue_count: null
              stack_type: IPV4_ONLY
              subnetwork: ((var.subnetwork_self_link_gpunet2))
              subnetwork_project: ((var.project_id))
            - access_config: []
              alias_ip_range: []
              ipv6_access_config: []
              network: null
              network_ip: ""
              nic_type: GVNIC
              queue_count: null
              stack_type: IPV4_ONLY
              subnetwork: ((var.subnetwork_self_link_gpunet3))
              subnetwork_project: ((var.project_id))
          bandwidth_tier: gvnic_enabled
          disable_public_ips: true
          disk_size_gb: ((var.disk_size_gb))
          disk_type: pd-ssd
          enable_smt: true
          instance_image:
            family: ((var.final_image_family))
            project: ((var.project_id))
          instance_image_custom: true
          labels: ((var.labels))
          machine_type: a3-highgpu-8g
          maintenance_interval: ((var.a3_maintenance_interval))
          node_conf:
            CoresPerSocket: 104
            Sockets: 2
          node_count_dynamic_max: 0
          node_count_static: 74
          on_host_maintenance: TERMINATE
          project_id: ((var.project_id))
          service_account:
            email: 874088236606-compute@developer.gserviceaccount.com
            scopes:
              - cloud-platform
      - source: community/modules/compute/schedmd-slurm-gcp-v5-partition
        kind: terraform
        id: a3_partition
        use:
          - a3_node_group
          - sysnet
          - homefs
        settings:
          deployment_name: ((var.deployment_name))
          enable_placement: false
          enable_reconfigure: ((var.enable_reconfigure))
          exclusive: false
          is_default: true
          network_storage: ((flatten([module.homefs.network_storage])))
          node_groups: ((flatten([module.a3_node_group.node_groups])))
          partition_conf:
            OverSubscribe: EXCLUSIVE
          partition_name: a3
          project_id: ((var.project_id))
          region: ((var.region))
          slurm_cluster_name: ((var.slurm_cluster_name))
          subnetwork_self_link: ((module.sysnet.subnetwork_self_link))
          zone: ((var.zone))
          zones: ((var.zones))
      - source: modules/scripts/startup-script
        kind: terraform
        id: controller_startup
        settings:
          deployment_name: ((var.deployment_name))
          labels: ((var.labels))
          project_id: ((var.project_id))
          region: ((var.region))
          runners:
            - destination: /tmp/scripts.tgz
              source: /tmp/scripts.tgz
              type: data
            - content: |
                #!/bin/bash
                tar --no-same-owner -xvf /tmp/scripts.tgz -C /opt/apps/
                mv /opt/apps/scripts/adm /opt/apps
                find /opt/apps/adm/ -type d -exec chmod 0755 {} +
                find /opt/apps/scripts/ -type d -exec chmod 0755 {} +
                find /opt/apps/scripts/ -type f -exec chmod 0755 {} +
                find /opt/apps/adm/slurm -type f -exec chmod 0755 {} +
              destination: stage_scripts.sh
              type: shell
            - content: |
                #!/bin/bash
                # reset enroot to defaults of files under /home and running under /run
                # allows basic enroot testing on login/controller nodes (reduced I/O)
                rm -f /etc/enroot/enroot.conf
              destination: reset_enroot.sh
              type: shell
      - source: community/modules/scheduler/schedmd-slurm-gcp-v5-controller
        kind: terraform
        id: slurm_controller
        use:
          - sysnet
          - a3_partition
          - debug_partition
          - homefs
        settings:
          cloud_parameters:
            no_comma_params: false
            resume_rate: 0
            resume_timeout: 900
            suspend_rate: 0
            suspend_timeout: 600
          compute_startup_script: |
            #!/bin/bash
            # Setup sudo for slurm job users
            mkdir -m 750 /var/slurm-sudoers.d
            (umask 117; echo '#includedir /var/slurm-sudoers.d' > /etc/sudoers.d/slurm-job-user)
            # Apply fix to common_account PAM config if not already applied.
            if ! grep -q '^account[[:blank:]].*[[:blank:]]pam_unix.so.*broken_shadow' /etc/pam.d/common-account; then
              sed -i '/^account[[:blank:]].*[[:blank:]]pam_unix.so[[:blank:]]*/ s/$/ broken_shadow/' /etc/pam.d/common-account
            fi
          controller_startup_script: ((module.controller_startup.startup_script))
          deployment_name: ((var.deployment_name))
          disk_size_gb: ((var.disk_size_gb))
          enable_cleanup_compute: ((var.enable_cleanup_compute))
          enable_cleanup_subscriptions: ((var.enable_cleanup_subscriptions))
          enable_reconfigure: ((var.enable_reconfigure))
          epilog_scripts:
            - content: |
                #!/bin/bash
                if [[ -x /opt/apps/adm/slurm/slurm_epilog ]]; then
                  exec /opt/apps/adm/slurm/slurm_epilog
                fi
              filename: external-epilog.sh
          instance_image:
            family: ((var.final_image_family))
            project: ((var.project_id))
          instance_image_custom: true
          labels: ((var.labels))
          machine_type: c2-standard-8
          network_self_link: ((module.sysnet.network_self_link))
          network_storage: ((flatten([module.homefs.network_storage])))
          partition: ((flatten([module.debug_partition.partition, flatten([module.a3_partition.partition])])))
          project_id: ((var.project_id))
          prolog_scripts:
            - content: |
                #!/bin/bash
                if [[ -x /opt/apps/adm/slurm/slurm_prolog ]]; then
                  exec /opt/apps/adm/slurm/slurm_prolog
                fi
              filename: external-prolog.sh
          region: ((var.region))
          slurm_cluster_name: ((var.slurm_cluster_name))
          slurm_conf_tpl: modules/embedded/community/modules/scheduler/schedmd-slurm-gcp-v5-controller/etc/long-prolog-slurm.conf.tpl
          subnetwork_self_link: ((module.sysnet.subnetwork_self_link))
          zone: ((var.zone))
      - source: community/modules/scheduler/schedmd-slurm-gcp-v5-login
        kind: terraform
        id: slurm_login
        use:
          - sysnet
          - slurm_controller
        settings:
          controller_instance_id: ((module.slurm_controller.controller_instance_id))
          deployment_name: ((var.deployment_name))
          disk_size_gb: ((var.disk_size_gb))
          disk_type: pd-balanced
          enable_reconfigure: ((var.enable_reconfigure))
          instance_image:
            family: ((var.final_image_family))
            project: ((var.project_id))
          instance_image_custom: true
          labels: ((var.labels))
          machine_type: c2-standard-16
          network_self_link: ((module.sysnet.network_self_link))
          project_id: ((var.project_id))
          pubsub_topic: ((module.slurm_controller.pubsub_topic))
          region: ((var.region))
          slurm_cluster_name: ((var.slurm_cluster_name))
          startup_script: |
            #!/bin/bash
            # reset enroot to defaults of files under /home and running under /run
            # allows basic enroot testing on login node (reduced I/O)
            rm -f /etc/enroot/enroot.conf
          subnetwork_self_link: ((module.sysnet.subnetwork_self_link))
          zone: ((var.zone))
terraform_backend_defaults:
  type: gcs
  configuration:
    bucket: geniac-tf-state-bucket
