The examples in this directory are used to show how enroot + pyxis can be used
to launch containerized workloads via Slurm.

Contents:

* `import_container.sh`: Uses enroot to create a squashfs container image.
* `build-nccl-tests.sh`: A Slurm batch script for building the nccl-tests.
* `run-nccl-tests.sh`: A Slurm batch script for running the nccl-tests
  `all_reduce_perf` benchmark.

# Running NCCL-Tests via Enroot/Pyxis

In general the workflow to deploy TCPX-enabled workloads via enroot-pyxis is
the following:

	1. Convert your container into a squashfs based container image
	2. Set required environment variables
	3. Run your application workload

## TLDR

For an end-to-end example, copy the `build-nccl-tests.sh` and
`run-nccl-tests.sh` to your login node.

And run the following:

	enroot import docker://nvcr.io#nvidia/pytorch:23.10-py3 # takes ~10 minutes
	sbatch build-nccl-tests.sh # takes ~4 minutes
	sbatch run-nccl-tests.sh # takes ~3 minutes

That should result in a slurm-XX.out file that contains the result of the nccl
`all_reduce_perf` benchmark:

	#                                                              out-of-place                       in-place
	#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
	#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)
	   268435456      67108864     float     sum      -1   6606.2   40.63   76.19    N/A   6677.1   40.20   75.38    N/A
	   536870912     134217728     float     sum      -1    12751   42.10   78.95    N/A    12742   42.13   79.00    N/A
	  1073741824     268435456     float     sum      -1    25856   41.53   77.86    N/A    26198   40.99   76.85    N/A
	  2147483648     536870912     float     sum      -1    50543   42.49   79.67    N/A    50495   42.53   79.74    N/A
	  4294967296    1073741824     float     sum      -1    97723   43.95   82.41    N/A    95952   44.76   83.93    N/A
	  8589934592    2147483648     float     sum      -1   189701   45.28   84.90    N/A   187167   45.89   86.05    N/A
	# Out of bounds values : 0 OK
	# Avg bus bandwidth    : 80.0772
	#

For more details, follow the remainder of this README.

## Convert container to squashfs image

All of the following should be done on the login node of your slurm cluster,
and while somewhere on the shared Filestore filesystem (typically the user's
home directory).

First we'll want to create a squash-fs version of the container we want to
launch. We do this because otherwise we'd be pulling the (typically >10GB)
image multiple times from the source on each node, converting to sqsh each
time, etc, which would make the job launch longer. For example, to use nvidia's
latest pytorch container, we run:

	enroot import docker://nvcr.io#nvidia/pytorch:23.10-py3

This will create a (large) file named "nvidia+pytorch+21.09-py3.sqsh". If we
were to launch this across many nodes, and the Filestore instance couldn't keep
up, we might want to consider pushing this .sqsh file up into GCS, then mount
GCS via gcs-fuse on the worker nodes. For now we'll assume Filestore will take
care of it. We should also do some testing of how much the startup time is
affected by each of these options.

## (Optional) Building your application with a custom NCCL

For building the nccl-tests binaries, we spawn a job that runs on the A3 nodes
within the same application container.  This may or may not be required for
your application depending on whether the binary is already built within the
container. For the nccl-tests, we mount a directory in our NFS /home filesystem
where weâ€™ll clone and then build the nccl-tests repository.  This can most
easily be done by using the `--container-mounts` flag.

	srun --container-mounts="$PWD:/nccl" \
            ...

Then the first step when you are building should be to cd to that working
directory, in this case "/nccl". Feel free to use a different working directory
if you prefer.

See build-nccl-tests.sh for an example.

## Running your application on A3 instances

For a complete example, run:

	sbatch run-nccl-tests.sh

The following sections details what needs to happen during each Slurm job:

	1. Set NCCL environment variables that wire up the multiple NICs and all other NCCL tuning parameters.
	2. Run your workload, making sure to user container-mounts to get
	   everything you need to communicate with TCPX on the host, and making
	   sure to set all the container environment variables to pass all the NCCL/tuning
	   parameters into the container runtime.

Earlier versions of the HPC Toolkit blueprint required the RxDM sidecar
container to be started and for the custom NCCL plugin to be installed. This is
no longer needed, as they are both taken care of by Slurm prolog/epilogs.

The RxDM sidecar container will be launched for all jobs that use more than 1
node.  See run-nccl-tests.sh for an example of how to structure your script.

### Set NCCL Environment Variables

The following variables are used to enable and optimize the NCCL / TCPX
communication. These are optimized for A3 instance types.

	# Only use TCPX for multi-node jobs.
        [[ "${SLURM_JOB_NUM_NODES}" -gt 1 ]] && export USE_TCPX=yes || export USE_TCPX=no

        # Only use TCPX for multi-node jobs.
        if [[ ${USE_TCPX} = "yes" ]]; then
          # Set up NCCL Environment variables
          export NCCL_NET=GPUDirectTCPX_v7
          # These network interfaces use Ubuntu's consistent naming scheme. See
          # https://manpages.ubuntu.com/manpages/focal/man7/systemd.net-naming-scheme.7.html
          export NCCL_SOCKET_IFNAME=enp0s12
          export NCCL_GPUDIRECTTCPX_CTRL_DEV=enp0s12
          export NCCL_GPUDIRECTTCPX_SOCKET_IFNAME=enp6s0,enp12s0,enp134s0,enp140s0
          export NCCL_CROSS_NIC=0
          export NCCL_ALGO=Ring
          export NCCL_PROTO=Simple
          export NCCL_NSOCKS_PERTHREAD=4
          export NCCL_SOCKET_NTHREADS=1
          export NCCL_MAX_NCHANNELS=12
          export NCCL_MIN_NCHANNELS=12
          export NCCL_DYNAMIC_CHUNK_SIZE=524288
          export NCCL_P2P_NET_CHUNKSIZE=524288
          export NCCL_P2P_PCI_CHUNKSIZE=524288
          export NCCL_P2P_NVL_CHUNKSIZE=1048576
          export NCCL_BUFFSIZE=4194304
          export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
          export NCCL_NET_GDR_LEVEL=PIX
          export NCCL_P2P_PXN_LEVEL=0
          export NCCL_GPUDIRECTTCPX_UNIX_CLIENT_PREFIX=${UDS_PATH}
          export NCCL_GPUDIRECTTCPX_PROGRAM_FLOW_STEERING_WAIT_MICROS=1000000
          export NCCL_GPUDIRECTTCPX_FORCE_ACK=0
          export NCCL_GPUDIRECTTCPX_TX_COMPLETION_NANOSLEEP=1000

          export LD_LIBRARY_PATH=/var/lib/tcpx/lib64:${LD_LIBRARY_PATH}
        else
          unset NCCL_NET
        fi

        # The following two can be useful for debugging
        # export NCCL_DEBUG=INFO
        # export NCCL_DEBUG_SUBSYS=INIT,GRAPH,ENV,TUNING

        # Here we grab all the environment variables that need to be
        # passed down into the container. Slurm would otherwise only pass these env vars
        # to the job environment on the host.
        HOST_VARS=`sed 's/ \{1,\}/,/g' <<< "${!USE_TCPX*} ${!NCCL*} LD_LIBRARY_PATH"`;

        # Mount /var/tmp to allow the rest of the enroot container to be read-only, and
        # mount current $PWD to /nccl to for accessing nccl-tests binary
        CONTAINER_MOUNTS="/var/tmp:/var/tmp"

        # Mount PWD to /nccl in the enroot container
        CONTAINER_MOUNTS=${CONTAINER_MOUNTS},"$PWD:/nccl"

        # Mount required directories for TCPX functionality
        if [[ ${USE_TCPX} = "yes" ]]; then
          CONTAINER_MOUNTS=${CONTAINER_MOUNTS},"/var/lib/tcpx/lib64:/var/lib/tcpx/lib64"
          CONTAINER_MOUNTS=${CONTAINER_MOUNTS},${UDS_PATH}:${UDS_PATH}
        fi

### Run your workload
Once the environment variables are set, we run our workload via Slurm / Pyxis

        # Run the workload
        srun --mpi=pmi2 \
          --cpu-bind=verbose \
          --export=ALL \
          --container-name=nccl \
          --container-image=${CONTAINER_IMAGE} \
          --container-env=${HOST_VARS} \
          --container-mounts=${CONTAINER_MOUNTS} \
          /nccl/nccl-tests/build/all_reduce_perf -b 1G -e 8G -f 2 -g 1 -w 5 --iters 40 -c 0

The pyxis slurm plugin has several specific flags that act as syntactic sugar to
launch container workloads on the worker nodes, specifically --container-image,
--container-env, and --container-mounts. The container mounts are particularly
important to get correct, as they enable the container application to properly
use TCPX.
