# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

blueprint_name: slurm-a3-image-tky
ghpc_version: v1.34.3-0-g627b43aa
vars:
  base_deployment_name: slurm-a3-base-tky
  deployment_name: slurm-a3-image-tky
  disk_size: 200
  final_image_family: slurm-dlvm
  labels:
    ghpc_blueprint: slurm-a3-image-tky
    ghpc_deployment: ((var.deployment_name))
  network_name_system: (("${var.base_deployment_name}-sysnet"))
  project_id: nii-geniac
  region: asia-northeast1
  slurm_cluster_name: slurm0tky
  source_image: dlvm-tcpd-cu120-20231203-1800-rc0-ubuntu-2004-py310
  source_image_project_id: deeplearning-platform
  subnetwork_name_system: (("${var.base_deployment_name}-sysnet-subnet"))
  zone: asia-northeast1-b
deployment_groups:
  - group: build_script
    terraform_backend:
      type: gcs
      configuration:
        bucket: geniac-tf-state-bucket
        prefix: (("slurm-a3-image-tky/${var.deployment_name}/build_script"))
    modules:
      - source: modules/network/pre-existing-vpc
        kind: terraform
        id: sysnet
        outputs:
          - name: subnetwork_name
            description: Automatically-generated output exported for use by later deployment groups
            sensitive: true
        settings:
          network_name: ((var.network_name_system))
          project_id: ((var.project_id))
          region: ((var.region))
          subnetwork_name: ((var.subnetwork_name_system))
      - source: modules/scripts/startup-script
        kind: terraform
        id: image_build_script
        outputs:
          - name: startup_script
            description: Automatically-generated output exported for use by later deployment groups
            sensitive: true
        settings:
          configure_ssh_host_patterns:
            - 10.4.0.*
            - 10.5.0.*
            - 10.6.0.*
            - 10.7.0.*
            - (("${var.slurm_cluster_name}*"))
          deployment_name: ((var.deployment_name))
          enable_docker_world_writable: true
          install_ansible: true
          install_docker: true
          labels: ((var.labels))
          project_id: ((var.project_id))
          region: ((var.region))
          runners:
            - content: |
                #!/bin/bash
                set -e -o pipefail
                rm -f /etc/apt/sources.list.d/kubernetes.list
                apt-get update --allow-releaseinfo-change
              destination: workaround_apt_change.sh
              type: shell
            - content: |
                #!/bin/bash
                # many extra services are being started via /etc/rc.local; disable
                # them on future boots of image
                echo -e '#!/bin/bash\n/usr/bin/nvidia-persistenced --user root\nexit 0' > /etc/rc.local
                # disable jupyter and notebooks-collection-agent services
                systemctl stop jupyter.service notebooks-collection-agent.service
                systemctl disable jupyter.service notebooks-collection-agent.service
              destination: disable_dlvm_builtin_services.sh
              type: shell
            - content: |
                {
                  "reboot": false,
                  "slurm_version": "23.02.7",
                  "install_cuda": false,
                  "nvidia_version": "latest",
                  "install_ompi": true,
                  "install_lustre": true,
                  "install_gcsfuse": true,
                  "monitoring_agent": "cloud-ops"
                }
              destination: /var/tmp/slurm_vars.json
              type: data
            - content: |
                #!/bin/bash
                set -e -o pipefail
                ansible-galaxy role install googlecloudplatform.google_cloud_ops_agents
                ansible-pull \
                    -U https://github.com/GoogleCloudPlatform/slurm-gcp -C 5.11.1 \
                    -i localhost, --limit localhost --connection=local \
                    -e @/var/tmp/slurm_vars.json \
                    ansible/playbook.yml
              destination: install_slurm.sh
              type: shell
            - content: |
                * - memlock unlimited
                * - nproc unlimited
                * - stack unlimited
                * - nofile 1048576
                * - cpu unlimited
                * - rtprio unlimited
              destination: /etc/security/limits.d/99-unlimited.conf
              type: data
            - content: |
                [Service]
                LimitNOFILE=infinity
              destination: /etc/systemd/system/slurmd.service.d/file_ulimit.conf
              type: data
            - content: |
                [Unit]
                Description=Delay A3 boot until all network interfaces are routable
                After=network-online.target
                Wants=network-online.target
                Before=google-startup-scripts.service

                [Service]
                ExecCondition=/bin/bash -c '/usr/bin/curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/machine-type | grep -q "/a3-highgpu-8g$"'
                ExecStart=/usr/lib/systemd/systemd-networkd-wait-online -i enp6s0 -i enp12s0 -i enp134s0 -i enp140s0 -o routable --timeout=120
                ExecStartPost=/bin/sleep 60

                [Install]
                WantedBy=multi-user.target
              destination: /etc/systemd/system/delay-a3.service
              type: data
            - content: |
                ENROOT_RUNTIME_PATH    /mnt/localssd/${UID}/enroot/runtime
                ENROOT_CACHE_PATH      /mnt/localssd/${UID}/enroot/cache
                ENROOT_DATA_PATH       /mnt/localssd/${UID}/enroot/data
              destination: /etc/enroot/enroot.conf
              type: data
            - content: |
                #!/bin/bash
                apt-get update
                apt-get install mdadm --no-install-recommends --assume-yes
              destination: install_mdadm.sh
              type: shell
            - content: |
                #!/bin/bash
                set -e -o pipefail

                RAID_DEVICE=/dev/md0
                DST_MNT=/mnt/localssd
                DISK_LABEL=LOCALSSD
                OPTIONS=discard,defaults

                # if mount is successful, do nothing
                if mount --source LABEL="$DISK_LABEL" --target="$DST_MNT" -o "$OPTIONS"; then
                        exit 0
                fi

                # Create new RAID, format ext4 and mount
                # TODO: handle case of zero or 1 local SSD disk
                # TODO: handle case when /dev/md0 exists but was not mountable for
                # some reason
                DEVICES=`nvme list | grep nvme_ | grep -v nvme_card-pd | awk '{print $1}' | paste -sd ' '`
                NB_DEVICES=`nvme list | grep nvme_ | grep -v nvme_card-pd | awk '{print $1}' | wc -l`
                mdadm --create "$RAID_DEVICE" --level=0 --raid-devices=$NB_DEVICES $DEVICES
                mkfs.ext4 -F "$RAID_DEVICE"
                tune2fs "$RAID_DEVICE" -r 131072
                e2label "$RAID_DEVICE" "$DISK_LABEL"
                mkdir -p "$DST_MNT"
                mount --source LABEL="$DISK_LABEL" --target="$DST_MNT" -o "$OPTIONS"
                chmod 1777 "$DST_MNT"
              destination: /usr/local/ghpc/mount_localssd.sh
              type: data
            - content: |
                [Unit]
                Description=Assemble local SSDs as software RAID; then format and mount

                [Service]
                ExecCondition=bash -c '/usr/bin/curl -s -H "Metadata-Flavor: Google" http://metadata.google.internal/computeMetadata/v1/instance/machine-type | grep -q "/a3-highgpu-8g$"'
                ExecStart=/bin/bash /usr/local/ghpc/mount_localssd.sh

                [Install]
                WantedBy=local-fs.target
              destination: /etc/systemd/system/mount-local-ssd.service
              type: data
            - content: |
                #!/bin/bash
                set -e -o pipefail
                apt-key del 7fa2af80
                distribution=\$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\.//g')
                wget https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/cuda-keyring_1.0-1_all.deb
                dpkg -i cuda-keyring_1.0-1_all.deb
                apt-get update
                apt-get install -y datacenter-gpu-manager
                # libnvidia-nscq needed for A100/A800 and H100/H800 systems
                apt-get install -y libnvidia-nscq-550
              destination: install_dcgm.sh
              type: shell
            - content: |
                #!/bin/bash
                tee -a /etc/google-cloud-ops-agent/config.yaml > /dev/null << EOF
                metrics:
                  receivers:
                    dcgm:
                      type: dcgm
                  service:
                    pipelines:
                      dcgm:
                        receivers:
                          - dcgm
                EOF
              destination: add_dcgm_to_op_config.sh
              type: shell
            - content: |
                #!/bin/bash
                set -e -o pipefail
                # workaround b/309016676 (systemd-resolved restarts 4 times causing DNS
                # resolution failures during google-startup-scripts.service)
                systemctl daemon-reload
                systemctl enable delay-a3.service
                systemctl enable mount-local-ssd.service
                systemctl enable nvidia-dcgm
              destination: systemctl_services.sh
              type: shell
            - content: |
                #!/bin/bash
                # THIS RUNNER MUST BE THE LAST RUNNER BECAUSE IT WILL BREAK GSUTIL IN
                # PARENT SCRIPT OF STARTUP-SCRIPT MODULE
                set -e -o pipefail
                # Remove original DLVM gcloud, lxds install due to conflict with snapd and NFS
                snap remove google-cloud-cli lxd
                # Install key and google-cloud-cli from apt repo
                GCLOUD_APT_SOURCE="/etc/apt/sources.list.d/google-cloud-sdk.list"
                if [ ! -f "${GCLOUD_APT_SOURCE}" ]; then
                    # indentation matters in EOT below; do not blindly edit!
                    cat <<EOT > "${GCLOUD_APT_SOURCE}"
                deb [signed-by=/usr/share/keyrings/cloud.google.asc] https://packages.cloud.google.com/apt cloud-sdk main
                EOT
                fi
                curl -o /usr/share/keyrings/cloud.google.asc https://packages.cloud.google.com/apt/doc/apt-key.gpg
                apt-get update
                apt-get install --assume-yes google-cloud-cli
                # Clean up the bash executable hash for subsequent steps using gsutil
                hash -r
              destination: remove_snap_gcloud.sh
              type: shell
  - group: slurm-build
    terraform_backend:
      type: gcs
      configuration:
        bucket: geniac-tf-state-bucket
        prefix: (("slurm-a3-image-tky/${var.deployment_name}/slurm-build"))
    modules:
      - source: modules/packer/custom-image
        kind: packer
        id: slurm-image
        use:
          - image_build_script
          - sysnet
        settings:
          deployment_name: ((var.deployment_name))
          disk_size: ((var.disk_size))
          image_family: ((var.final_image_family))
          labels: ((var.labels))
          machine_type: c2-standard-30
          project_id: ((var.project_id))
          source_image: ((var.source_image))
          source_image_project_id:
            - ((var.source_image_project_id))
          startup_script: ((module.image_build_script.startup_script))
          subnetwork_name: ((module.sysnet.subnetwork_name))
          zone: ((var.zone))
terraform_backend_defaults:
  type: gcs
  configuration:
    bucket: geniac-tf-state-bucket
